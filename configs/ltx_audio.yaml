model_name: "ltx_lora"
id_token: "sounding object, "
data_root: "/mnt/sda1/saksham/TI2AV/others/ltx_lora_training_i2v_t2v/cache_121x768x512" # one type of resolution and num_frames; if you set precomputed_datasets this line will be ignored
precomputed_datasets: # a list of folder contain defferient resolutions and frames
  - "/mnt/sda1/saksham/TI2AV/others/ltx_lora_training_i2v_t2v/cache_audio" # TODO

output_dir: "/mnt/sda1/saksham/TI2AV/others/ltx_lora_training_i2v_t2v/mixed"
caption_dropout_p: 0.06
caption_dropout_technique: "zero" # zero, phrase
dataloader_num_workers: 4
pretrained_model_name_or_path: "a-r-r-o-w/LTX-Video-0.9.1-diffusers"

# if you want to resume from prev checkpoint
# prev_checkpoint: "/home/eisneim/www/ml/_video_gen/ltx_training/data/mixed/checkpoint-2000"

flow_resolution_shifting: True
seed: 7136

height: 512
width: 768

# mixed_precision: "no"
# batch_size: 4

mixed_precision: bf16
batch_size: 6
train_epochs: 200

is_i2v: True
noise_to_first_frame: 0.2
train_type: "lora"
rank: 256
lora_alpha: 256
# target_modules: to_q to_k to_v to_out.0
target_modules: "all-linear"
gradient_accumulation_steps: 1
gradient_checkpointing: True

checkpointing_steps: 1000
checkpointing_limit: 4
enable_slicing: True
enable_tiling: True

optimizer_8bit: True
# optimizer_torchao: True
optimizer: adamw
lr: 2e-5
lr_scheduler: constant_with_warmup
lr_warmup_steps: 100
lr_num_cycles: 1
beta1: 0.9
beta2: 0.95
weight_decay: 1e-4
epsilon: 1e-8
max_grad_norm: 1.0

validation_prompts: ""
num_validation_videos: 1
validation_steps: 100000000000000000 # don't do validation

tracker_name: ltx_trainer
nccl_timeout: 1800